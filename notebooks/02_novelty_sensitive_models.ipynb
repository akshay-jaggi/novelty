{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db3f869-d48d-48f3-a8e4-e18c584f3c32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from novelty.models.dists import benford_dist, instrumental_dist\n",
    "from novelty.models.mlp import MLP\n",
    "from novelty.models.util import train_streaming_unbalanced, test\n",
    "from novelty.visualization.models import plot_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f7a131-cad1-4403-9d0c-599d0124edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = False\n",
    "device = torch.device(\"mps\" if use_gpu else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783b5227-9c97-42a4-9737-92c53608d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "train_kwargs = {'batch_size': 1}\n",
    "test_kwargs = {'batch_size': 1000}\n",
    "if use_gpu:\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                   'pin_memory': True,\n",
    "                   'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # image mean and std \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83da1cc1-d3b9-4116-a62d-e65afc4a2be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = datasets.MNIST('../data/raw', train=True, download=True,\n",
    "                   transform=transform)\n",
    "dataset2 = datasets.MNIST('../data/raw', train=False,\n",
    "                   transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7f21de-dc2c-441f-9c91-0af6e8fd59ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "benford_probs = benford_dist(torch.arange(0,10))\n",
    "max_scaler = max(benford_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fe7c52-ac7f-4e59-af7d-ed91422fecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_func_inverse(prob):\n",
    "    return 0.001 * (1/prob) \n",
    "\n",
    "def prob_func_log(prob):\n",
    "    return 0.01 * -torch.log(prob) / torch.log(torch.tensor(11))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acca6c56-ddb2-4bb3-aa59-801ec823369d",
   "metadata": {},
   "source": [
    "# Naive Novelty Based Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87360e0-2eb9-4946-af3d-dd40b52e5ae7",
   "metadata": {},
   "source": [
    "Knowing that the samples are drawn from the Benford Distribution, we can attempt to scale our learning rate by the inverse of the probabilty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e533a63-159c-4f03-802e-15225f48f1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_per_epoch = 10000\n",
    "batches_per_epoch = examples_per_epoch//train_kwargs['batch_size']\n",
    "epochs = len(train_loader.dataset)//examples_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05073882-338e-4f1e-8c6a-07b9b36ed6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21717e0e-bdfc-4a9f-a1fa-e54c13b344df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accs = []\n",
    "train_kept = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "for epoch in tqdm(range(1, epochs + 1)):\n",
    "    train_loss, train_acc, kept = train_streaming_unbalanced(model, device, train_loader, optimizer, epoch, \n",
    "                                                       benford_dist, instrumental_dist, max_scaler, \n",
    "                                                       batches_per_epoch, True, prob_func_inverse)\n",
    "    test_loss, test_acc = test(model, device, test_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    train_kept.append(kept)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67408e8-782d-4f1d-9624-8d2ac32dd371",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accs(train_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a37dcb-b1b5-409c-8abe-e59e5bbf1294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
