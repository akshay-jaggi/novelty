{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9f0720-e3d9-4060-904a-a1734d745856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from novelty.models.dists import benford_dist, instrumental_dist\n",
    "from novelty.models.mlp import MLP\n",
    "from novelty.models.util import train, train_streaming, train_streaming_unbalanced, test\n",
    "from novelty.visualization.models import plot_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21a9a01-8e15-4ded-900e-59ac09cd4c44",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Vanilla Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5909d220-020c-4d0f-8f58-cec5747ae07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = False\n",
    "device = torch.device(\"mps\" if use_gpu else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5810a4d1-24cb-4a16-ad47-741c5b14343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "train_kwargs = {'batch_size': 100}\n",
    "test_kwargs = {'batch_size': 1000}\n",
    "if use_gpu:\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                   'pin_memory': True,\n",
    "                   'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # image mean and std \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cd3631-48aa-47f7-9253-a7b52f82e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = datasets.MNIST('../data/raw', train=True, download=True,\n",
    "                   transform=transform)\n",
    "dataset2 = datasets.MNIST('../data/raw', train=False,\n",
    "                   transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de984b37-d269-4f43-9290-a5f68ebf8a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP().to(device)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d41f11-c63f-4c38-8c64-5b1e56fdbb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accs = []\n",
    "train_kept = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "for epoch in tqdm(range(1, epochs + 1)):\n",
    "    train_loss, train_acc, kept = train(model, device, train_loader, optimizer)\n",
    "    test_loss, test_acc = test(model, device, test_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    train_kept.append(kept)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19195713-dc99-4bf5-9a7a-ad9627c2ff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accs(train_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4fd520-3c79-43f9-8d88-1586e3ca62eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accs(test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919c1f37-feb5-40b0-8b8f-12bd810dd4c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Streaming Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac5aa67-9a0b-4c7b-b817-9166d602a16c",
   "metadata": {},
   "source": [
    "In a datastreaming context, we assume that the model can only see each of the 60000 training examples once. Therefore, we'd like to maximize the information gain from each training example. We'll still use 10 epochs, but each epoch will only be 10000 examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b379ba5-926e-4d5a-9253-24d76b437d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_per_epoch = 10000\n",
    "batches_per_epoch = examples_per_epoch//train_kwargs['batch_size']\n",
    "epochs = len(train_loader.dataset)//examples_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8398fb40-8635-4b57-99db-bc7152b0feff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP().to(device)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6d8b67-3e2f-40d7-9d52-5e05470dfbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accs = []\n",
    "train_kept = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "for epoch in tqdm(range(1, epochs + 1)):\n",
    "    train_loss, train_acc, kept = train_streaming(model, device, train_loader, optimizer, epoch)\n",
    "    test_loss, test_acc = test(model, device, test_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    train_kept.append(kept)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcd861c-8847-4b8e-978f-4a07c2938b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accs(train_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f06754f-3aed-46eb-9767-7868ba490b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accs(test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc410257-09c0-48e7-a63f-5e47532d76cf",
   "metadata": {},
   "source": [
    "# Unbalanced Streaming Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a747cd-6e49-43c6-8f7f-f797d36b1208",
   "metadata": {},
   "source": [
    "Unlike in the previous streaming context, we'll use rejection sampling to create an unbalanced distribution over the 10 mnist letters as they stream in. The goal here will be to ensure that model sees more 0s than 1s than 2s etc. For an initial probability distribution, I'll use Benford's law, which is based on the frequency of first digits in data: P(d) = log10(1 + 1/d) . Because we have 10 (rather than 9) digits, we'll use P(d) = log11(1+1/d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c705f5e0-ed34-4134-83f8-499915dd94cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "benford_probs = benford_dist(torch.arange(0,10))\n",
    "max_scaler = max(benford_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc06c75-88d0-4264-8935-cd19d8981808",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP().to(device)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c56107c-25b9-481a-bb2b-8bcc670ae108",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accs = []\n",
    "train_kept = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "for epoch in tqdm(range(1, epochs + 1)):\n",
    "    train_loss, train_acc, kept = train_streaming_unbalanced(model, device, train_loader, optimizer, epoch, \n",
    "                                                       benford_dist, instrumental_dist, max_scaler, \n",
    "                                                       batches_per_epoch)\n",
    "    test_loss, test_acc = test(model, device, test_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    train_kept.append(kept)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73010f94-f866-4535-8ec3-5881a3a966f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accs(train_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b04a189-3c22-4e11-ad71-6d85b1fe9c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accs(test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df44ebd-ba72-45f9-ae94-ec3ce3bd000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.stack(train_kept).sum(axis=0))\n",
    "plt.ylabel('frequency')\n",
    "plt.xlabel('digit')\n",
    "plt.xticks(torch.arange(0,10))\n",
    "plt.ylim(0,7000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d586420c-add9-471c-99cf-0ea9879722d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(benford_probs)\n",
    "plt.ylabel('probability')\n",
    "plt.xlabel('digit')\n",
    "plt.xticks(torch.arange(0,10))\n",
    "plt.ylim(0,0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf47d47f-3506-4d2e-8f7b-bb668e62ba92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
