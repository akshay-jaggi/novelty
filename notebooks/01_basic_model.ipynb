{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da9f0720-e3d9-4060-904a-a1734d745856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshay/opt/anaconda3/envs/novelty/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from novelty.models.dists import benford_dist, instrumental_dist\n",
    "from novelty.models.mlp import MLP\n",
    "from novelty.models.util import train, train_streaming, train_streaming_unbalanced, test\n",
    "from novelty.visualization.models import plot_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21a9a01-8e15-4ded-900e-59ac09cd4c44",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Vanilla Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5909d220-020c-4d0f-8f58-cec5747ae07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = False\n",
    "device = torch.device(\"mps\" if use_gpu else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5810a4d1-24cb-4a16-ad47-741c5b14343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "train_kwargs = {'batch_size': 100}\n",
    "test_kwargs = {'batch_size': 1000}\n",
    "if use_gpu:\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                   'pin_memory': True,\n",
    "                   'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # image mean and std \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49cd3631-48aa-47f7-9253-a7b52f82e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = datasets.MNIST('../data/raw', train=True, download=True,\n",
    "                   transform=transform)\n",
    "dataset2 = datasets.MNIST('../data/raw', train=False,\n",
    "                   transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de984b37-d269-4f43-9290-a5f68ebf8a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP().to(device)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8d41f11-c63f-4c38-8c64-5b1e56fdbb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████████▌                                                                                                                          | 1/10 [00:10<01:37, 10.86s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m test_accs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m----> 7\u001b[0m     train_loss, train_acc, kept \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m test(model, device, test_loader)\n\u001b[1;32m      9\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "File \u001b[0;32m~/Documents/Harvard-MIT/Rotations/Yang/novelty/novelty/models/util.py:37\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer)\u001b[0m\n\u001b[1;32m     35\u001b[0m kept \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m---> 37\u001b[0m     train_loss, correct, kept \u001b[38;5;241m=\u001b[39m \u001b[43mminibatch_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkept\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrect\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_loss \u001b[38;5;241m/\u001b[39m kept\u001b[38;5;241m.\u001b[39msum(), correct \u001b[38;5;241m/\u001b[39m kept, kept\n",
      "File \u001b[0;32m~/Documents/Harvard-MIT/Rotations/Yang/novelty/novelty/models/util.py:23\u001b[0m, in \u001b[0;36mminibatch_step\u001b[0;34m(model, device, optimizer, data, target, train_loss, kept, correct)\u001b[0m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     22\u001b[0m loss, pred \u001b[38;5;241m=\u001b[39m forward_pass(model, data, target)\n\u001b[0;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     25\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_loss \u001b[38;5;241m+\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/novelty/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/novelty/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "train_accs = []\n",
    "train_kept = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "for epoch in tqdm(range(1, epochs + 1)):\n",
    "    train_loss, train_acc, kept = train(model, device, train_loader, optimizer)\n",
    "    test_loss, test_acc = test(model, device, test_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    train_kept.append(kept)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19195713-dc99-4bf5-9a7a-ad9627c2ff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accs(train_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4fd520-3c79-43f9-8d88-1586e3ca62eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accs(test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919c1f37-feb5-40b0-8b8f-12bd810dd4c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Streaming Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac5aa67-9a0b-4c7b-b817-9166d602a16c",
   "metadata": {},
   "source": [
    "In a datastreaming context, we assume that the model can only see each of the 60000 training examples once. Therefore, we'd like to maximize the information gain from each training example. We'll still use 10 epochs, but each epoch will only be 10000 examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b379ba5-926e-4d5a-9253-24d76b437d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_per_epoch = 10000\n",
    "batches_per_epoch = examples_per_epoch//train_kwargs['batch_size']\n",
    "epochs = len(train_loader.dataset)//examples_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8398fb40-8635-4b57-99db-bc7152b0feff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e6d8b67-3e2f-40d7-9d52-5e05470dfbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:24<00:00,  4.08s/it]\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "train_accs = []\n",
    "train_kept = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "for epoch in tqdm(range(1, epochs + 1)):\n",
    "    train_loss, train_acc, kept = train_streaming(model, device, train_loader, optimizer, epoch)\n",
    "    test_loss, test_acc = test(model, device, test_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    train_kept.append(kept)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdcd861c-8847-4b8e-978f-4a07c2938b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4861, 0.7104, 0.8277, 0.8513, 0.8646, 0.8950])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(train_accs).mean(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f06754f-3aed-46eb-9767-7868ba490b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accs(test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc410257-09c0-48e7-a63f-5e47532d76cf",
   "metadata": {},
   "source": [
    "# Unbalanced Streaming Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a747cd-6e49-43c6-8f7f-f797d36b1208",
   "metadata": {},
   "source": [
    "Unlike in the previous streaming context, we'll use rejection sampling to create an unbalanced distribution over the 10 mnist letters as they stream in. The goal here will be to ensure that model sees more 0s than 1s than 2s etc. For an initial probability distribution, I'll use Benford's law, which is based on the frequency of first digits in data: P(d) = log10(1 + 1/d) . Because we have 10 (rather than 9) digits, we'll use P(d) = log11(1+1/d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c705f5e0-ed34-4134-83f8-499915dd94cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "benford_probs = benford_dist(torch.arange(0,10))\n",
    "max_scaler = max(benford_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc06c75-88d0-4264-8935-cd19d8981808",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP().to(device)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c56107c-25b9-481a-bb2b-8bcc670ae108",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accs = []\n",
    "train_kept = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "for epoch in tqdm(range(1, epochs + 1)):\n",
    "    train_loss, train_acc, kept = train_streaming_unbalanced(model, device, train_loader, optimizer, epoch, \n",
    "                                                       benford_dist, instrumental_dist, max_scaler, \n",
    "                                                       batches_per_epoch)\n",
    "    test_loss, test_acc = test(model, device, test_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    train_kept.append(kept)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73010f94-f866-4535-8ec3-5881a3a966f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accs(train_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b04a189-3c22-4e11-ad71-6d85b1fe9c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accs(test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df44ebd-ba72-45f9-ae94-ec3ce3bd000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.stack(train_kept).sum(axis=0))\n",
    "plt.ylabel('frequency')\n",
    "plt.xlabel('digit')\n",
    "plt.xticks(torch.arange(0,10))\n",
    "plt.ylim(0,7000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d586420c-add9-471c-99cf-0ea9879722d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(benford_probs)\n",
    "plt.ylabel('probability')\n",
    "plt.xlabel('digit')\n",
    "plt.xticks(torch.arange(0,10))\n",
    "plt.ylim(0,0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf47d47f-3506-4d2e-8f7b-bb668e62ba92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
